{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-military",
   "metadata": {},
   "source": [
    "# Developing the Text Generation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-credit",
   "metadata": {},
   "source": [
    "First of all, the text generation that I will be using requires a prompt. For that I'll take the first 10 words of the 27k+ rows of data I have, and use those for the prompts. I'm okay with 10 characters being \"predetermined\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/datasetv2.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-camel",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "Hugging Face Transformers makes it really easy to load pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = df.description.apply(lambda x: \" \".join(x.split()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = random.choice(prompts)\n",
    "prompt_encoded = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(\n",
    "    prompt_encoded,\n",
    "    do_sample=True, \n",
    "    max_length=500, \n",
    "    top_k=50, \n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=5,\n",
    ")\n",
    "output_decoded = tokenizer.decode(output[0])\n",
    "\n",
    "\n",
    "print(prompt)\n",
    "print(output_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-launch",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Model\n",
    "\n",
    "The output already looks fantastic, but let's fine-tune the model to get even better results.\n",
    "\n",
    "I'm going to work on the video game name generation first as a POC. We can use a special token, for example `<|name|>` as a prompt instead of needing words to prompt the title generation.\n",
    " \n",
    "See: https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a\n",
    "\n",
    "So all we really need to do is format our data in with the prompt token (for this task, `<|name|>`) and the end of text token, which is built into the pretrained tokenizer: `<|endoftext|>` and fine-tune the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_TOKEN = \"<|name|>\"\n",
    "END_TOKEN = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example formatted name\n",
    "name = df.name[0]\n",
    "formatted_name = f\"{NAME_TOKEN}{name}{END_TOKEN}\"\n",
    "print(formatted_name)\n",
    "print(tokenizer.encode(formatted_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_formatted(file, list_of_texts, start_token, end_token):\n",
    "    for text in list_of_texts:\n",
    "        formatted_text = f\"{start_token}{text}{end_token}\"\n",
    "        file.write(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into train and validation\n",
    "train, validation = train_test_split(df.name, train_size=0.85, random_state=42)\n",
    "\n",
    "print(\"train count:\", train.count())\n",
    "print(\"validation count:\", validation.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/training/name_train.txt\", \"w\") as f:\n",
    "    save_formatted(f, train, NAME_TOKEN, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/training/name_val.txt\", \"w\") as f:\n",
    "    save_formatted(f, validation, NAME_TOKEN, END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caroline-reward",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2021 16:50:23 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "03/30/2021 16:50:23 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=../data/models/gpt2-name/, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Mar30_16-50-23_jc-ps63, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=../data/models/gpt2-name/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1)\n",
      "03/30/2021 16:50:23 - WARNING - datasets.builder -   Using custom data configuration default-cbcc67979657cbff\n",
      "03/30/2021 16:50:23 - WARNING - datasets.builder -   Reusing dataset text (/home/jason/.cache/huggingface/datasets/text/default-cbcc67979657cbff/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "[INFO|configuration_utils.py:472] 2021-03-30 16:50:23,675 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:508] 2021-03-30 16:50:23,677 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:472] 2021-03-30 16:50:23,855 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:508] 2021-03-30 16:50:23,857 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.5.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,954 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,955 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,956 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,956 >> loading file https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,956 >> loading file https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1702] 2021-03-30 16:50:24,956 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1051] 2021-03-30 16:50:25,240 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1167] 2021-03-30 16:50:34,785 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1175] 2021-03-30 16:50:34,785 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3131] 2021-03-30 16:50:35,295 >> Token indices sequence length is longer than the specified maximum sequence length for this model (254633 > 1024). Running this sequence through the model will result in indexing errors\n",
      "03/30/2021 16:50:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/text/default-cbcc67979657cbff/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-0f17653efbe0f277.arrow\n",
      "03/30/2021 16:50:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/text/default-cbcc67979657cbff/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-d9122004b55fab27.arrow\n",
      "03/30/2021 16:50:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/text/default-cbcc67979657cbff/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-102bac4a49182bf0.arrow\n",
      "03/30/2021 16:50:35 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/text/default-cbcc67979657cbff/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-3cf24b1e5574c93d.arrow\n",
      "[INFO|trainer.py:396] 2021-03-30 16:50:38,147 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:617] 2021-03-30 16:50:38,149 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:617] 2021-03-30 16:50:38,152 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:988] 2021-03-30 16:50:38,152 >> ***** Running training *****\n",
      "[INFO|trainer.py:989] 2021-03-30 16:50:38,152 >>   Num examples = 248\n",
      "[INFO|trainer.py:990] 2021-03-30 16:50:38,152 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:991] 2021-03-30 16:50:38,152 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:992] 2021-03-30 16:50:38,152 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:993] 2021-03-30 16:50:38,152 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:994] 2021-03-30 16:50:38,152 >>   Total optimization steps = 1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1240 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/jason/projects/content/tgdne/notebooks/../scripts/run_clm.py\", line 444, in <module>\n",
      "    main()\n",
      "  File \"/home/jason/projects/content/tgdne/notebooks/../scripts/run_clm.py\", line 409, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/trainer.py\", line 1095, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/trainer.py\", line 1483, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/trainer.py\", line 1517, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 904, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 752, in forward\n",
      "    outputs = block(\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 290, in forward\n",
      "    attn_outputs = self.attn(\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 241, in forward\n",
      "    attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\", line 183, in _attn\n",
      "    w = self.attn_dropout(w)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/home/jason/miniconda3/envs/tgdne/lib/python3.9/site-packages/torch/nn/functional.py\", line 1076, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 3.95 GiB total capacity; 3.00 GiB already allocated; 53.50 MiB free; 3.00 GiB reserved in total by PyTorch)\n",
      "  0%|                                                  | 0/1240 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/run_clm.py \\\n",
    "--model_type \"gpt2-medium\" \\\n",
    "--model_name_or_path \"gpt2-medium\" \\\n",
    "--train_file \"../data/training/name_train.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"../data/training/name_val.txt\" \\\n",
    "--do_eval \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--output_dir \"../data/models/gpt2-name/\" \\\n",
    "--per_gpu_train_batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-ecology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
